{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c881241f-71bc-4bad-9611-d44b06c3bc55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Define colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 150, 0)  # Darker shade of green\n",
    "RED = (255, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "# Gridworld dimensions\n",
    "n = 5  # Number of rows\n",
    "m = 5  # Number of columns\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((650, 700))  # Set screen size to 650x700 to accommodate log area and wider grid\n",
    "pygame.display.set_caption(\"Gridworld Reinforcement Learning\")\n",
    "\n",
    "# Font for text display\n",
    "font = pygame.font.SysFont('comicsansms', 20)  # Change font to Arial and reduce size for better fit\n",
    "\n",
    "# Cell size\n",
    "cell_size = 120  # Increase cell size for better visibility\n",
    "\n",
    "# Logging\n",
    "log = []\n",
    "\n",
    "def draw_gridworld(grid, V=None):\n",
    "    \"\"\"\n",
    "    Draws the gridworld on the screen with rewards, values (optional), and log entries.\n",
    "    \"\"\"\n",
    "    screen.fill(WHITE)  # Fill screen with white color\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            # Draw border around each cell\n",
    "            pygame.draw.rect(screen, BLACK, (j * cell_size, i * cell_size, cell_size, cell_size), 1)\n",
    "\n",
    "            # Color cells based on grid values\n",
    "            if grid[i, j] == 'X':  # Obstacle\n",
    "                color = BLACK\n",
    "                pygame.draw.rect(screen, color, (j * cell_size, i * cell_size, cell_size, cell_size))  # Draw obstacle background\n",
    "            elif grid[i, j] == '1':  # Goal\n",
    "                color = GREEN\n",
    "                pygame.draw.rect(screen, GREEN, (j * cell_size + 1, i * cell_size + 1, cell_size - 2, cell_size - 2))  # Dark green for goal state\n",
    "                \n",
    "            else:\n",
    "                color = WHITE\n",
    "\n",
    "            # Draw value on top of the grid \n",
    "            if V is not None:\n",
    "                value_text = font.render(str(round(V[i, j], 2)), True, BLACK)\n",
    "                text_rect = value_text.get_rect(center=(j * cell_size + cell_size // 2, i * cell_size + cell_size // 2))  # Center-align text\n",
    "                screen.blit(value_text, text_rect)\n",
    "\n",
    "            # Draw reward text in the center of the cell\n",
    "            reward_text = font.render(str(grid[i, j]), True, color)\n",
    "            reward_text_rect = reward_text.get_rect(center=(j * cell_size + cell_size // 2, i * cell_size + cell_size // 2))\n",
    "            screen.blit(reward_text, reward_text_rect)\n",
    "\n",
    "    # Draw log text\n",
    "    log_text = font.render(\"Log:\", True, BLACK)\n",
    "    screen.blit(log_text, (10, n * cell_size + 20))\n",
    "    log_area_height = 150\n",
    "    log_area = pygame.Rect(10, (n * cell_size + 50), 630, log_area_height)  # Define log area rectangle\n",
    "    pygame.draw.rect(screen, WHITE, log_area)  # Draw log area background\n",
    "    for index, entry in enumerate(log[-5:]):  # Display only the last 5 log entries\n",
    "        log_entry = font.render(entry, True, BLACK)\n",
    "        screen.blit(log_entry, (20, (n * cell_size + 70) + index * 25))\n",
    "\n",
    "def draw_agent(agent_pos):\n",
    "    \"\"\"\n",
    "    Draws the agent on the screen.\n",
    "    \"\"\"\n",
    "    pygame.draw.circle(screen, RED, (agent_pos[1] * cell_size + cell_size // 2,\n",
    "                                     agent_pos[0] * cell_size + cell_size // 2), cell_size // 3)\n",
    "    # Draw a dot in the center of the agent's cell\n",
    "    pygame.draw.circle(screen, BLACK, (agent_pos[1] * cell_size + cell_size // 2,\n",
    "                                       agent_pos[0] * cell_size + cell_size // 2), 3)\n",
    "\n",
    "def get_valid_actions(grid, state):\n",
    "    \"\"\"\n",
    "    Returns a list of valid actions from the given state (considering obstacles).\n",
    "    \"\"\"\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    valid_actions = []\n",
    "    for action in actions:\n",
    "        new_pos = get_next_state(state, action)\n",
    "        if new_pos is not None and grid[new_pos[0], new_pos[1]] != 'X':\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions\n",
    "\n",
    "def value_iteration(grid, start, goal, discount_factor=0.9):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to calculate the state value table, addressing invalid moves.\n",
    "    \"\"\"\n",
    "    n = grid.shape[0]\n",
    "    m = grid.shape[1]\n",
    "    V = np.zeros((n, m))\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i, j] == 'X':\n",
    "                    continue\n",
    "\n",
    "                v_old = V[i, j]\n",
    "                expected_rewards = []\n",
    "                valid_actions = get_valid_actions(grid, (i, j))\n",
    "\n",
    "                # Reward based on distance to goal (replace with your desired reward function)\n",
    "                distance_to_goal = np.abs(i - (n - 1)) + np.abs(j - (m - 1))\n",
    "                reward = 1.0 - distance_to_goal / ((n - 1) + (m - 1))  # Higher reward closer to goal\n",
    "\n",
    "                # Only consider valid actions to avoid invalid transitions\n",
    "                for action in valid_actions:\n",
    "                    new_pos = get_next_state((i, j), action)\n",
    "                    if new_pos == goal:\n",
    "                        expected_rewards.append(0)  # Goal state has no immediate reward\n",
    "                    elif new_pos == (i, j):  # Penalize revisiting the same state (increased penalty)\n",
    "                        expected_rewards.append(-1.0)  # Adjust penalty as needed\n",
    "                    else:\n",
    "                        expected_rewards.append(reward + discount_factor * V[new_pos[0], new_pos[1]])\n",
    "\n",
    "                # Update value based on valid actions only\n",
    "                if len(expected_rewards) > 0:\n",
    "                    V[i, j] = np.max(expected_rewards)\n",
    "                else:\n",
    "                    # Handle states with no valid actions (e.g., surrounded by obstacles)\n",
    "                    V[i, j] = v_old  # Maintain current value (avoid negative updates)\n",
    "\n",
    "                delta = max(delta, abs(v_old - V[i, j]))\n",
    "\n",
    "        if delta < 1e-8:  # Epsilon for convergence criteria (adjust as needed)\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"\n",
    "    Returns the next state after taking an action from the current state,\n",
    "    handling boundary conditions.\n",
    "    \"\"\"\n",
    "    if action == 'up':\n",
    "        new_row_index = max(state[0] - 1, 0)\n",
    "        new_col_index = state[1]\n",
    "    elif action == 'down':\n",
    "        new_row_index = min(state[0] + 1, n - 1)\n",
    "        new_col_index = state[1]\n",
    "    elif action == 'left':\n",
    "        new_row_index = state[0]\n",
    "        new_col_index = max(state[1] - 1, 0)\n",
    "    elif action == 'right':\n",
    "        new_row_index = state[0]\n",
    "        new_col_index = min(state[1] + 1, m - 1)\n",
    "\n",
    "    return (new_row_index, new_col_index)\n",
    "\n",
    "def get_action_index(action):\n",
    "    \"\"\"\n",
    "    Returns the index corresponding to a given action.\n",
    "    \"\"\"\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    return actions.index(action)\n",
    "\n",
    "# Placeholder grid (modify with appropriate rewards)\n",
    "grid = np.array([\n",
    "    [0.2, 0.3, 0.8, 0.1, 0],\n",
    "    [0.5, 'X', 0.7, 0.4, 0],\n",
    "    [0.9, 0.1, 0.6, 'X', 0.2],\n",
    "    [0.4, 'X', 0.3, 0.5, 0.8],\n",
    "    [0.7, 0.6, 0.1, 0.9, 1]  # Setting the goal state to 1\n",
    "])\n",
    "\n",
    "# Main loop\n",
    "running = True\n",
    "agent_pos = (0, 0)  # Initial agent position\n",
    "training_step_frequency = 10  # Train every 10 frames\n",
    "training_active = True  # Flag for training (optional)\n",
    "frame_count = 0\n",
    "\n",
    "# Additional variables for saving the model\n",
    "policy = {}  # Dictionary to store optimal actions for each state (learned from Value Iteration)\n",
    "model_saved = False  # Flag to track if the model is saved\n",
    "\n",
    "epsilon = 0.5  # Initial epsilon for exploration (adjust as needed)\n",
    "decay_rate = 0.95  # Initial decay rate for epsilon (adjust as needed)  # Steeper initial decay\n",
    "\n",
    "status_text = font.render(\"Training Status:\", True, BLACK)\n",
    "\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Update logic (separate training and visualization)\n",
    "    if training_active:\n",
    "        # Perform Value Iteration training step\n",
    "        V = value_iteration(grid.copy(), start=(0, 0), goal=(n - 1, m - 1))\n",
    "\n",
    "        # Extract optimal policy from the state value table (V)\n",
    "        policy = {}\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i, j] == 'X':\n",
    "                    continue\n",
    "                valid_actions = get_valid_actions(grid, (i, j))\n",
    "                if random.random() < epsilon:  # Epsilon-greedy strategy\n",
    "                    best_action = random.choice(valid_actions)  # Randomly select an action (exploration)\n",
    "                else:\n",
    "                    # Get action indices within bounds\n",
    "                    valid_action_indices = [get_action_index(action) for action in valid_actions]\n",
    "                    valid_action_indices = [idx for idx in valid_action_indices if j + idx < m]\n",
    "                    best_action_index = np.argmax([V[i, j + idx] for idx in valid_action_indices])\n",
    "                    best_action = valid_actions[best_action_index]\n",
    "                policy[(i, j)] = best_action  # Store optimal action for the state\n",
    "\n",
    "        # Adjust epsilon decay (steeper initial decay, slower later)\n",
    "        epsilon *= max(0.1, decay_rate)  # Ensure epsilon doesn't reach zero\n",
    "        decay_rate *= 0.99  # Gradual decay after initial phase\n",
    "\n",
    "        # Choose action based on the learned policy\n",
    "        current_action = policy.get(agent_pos, random.choice(['up', 'down', 'left', 'right']))  # Access action from policy dictionary\n",
    "\n",
    "        # Update agent position based on the action\n",
    "        new_pos = get_next_state(agent_pos, current_action)\n",
    "        if new_pos != agent_pos:  # Check if the agent's position has changed\n",
    "            agent_pos = new_pos\n",
    "            # Log action, reward, and current state\n",
    "            reward = grid[agent_pos[0], agent_pos[1]]  # Obtain reward from grid\n",
    "            log.append(f\"Action: {current_action}, Reward: {reward}, Current State: {agent_pos}\")\n",
    "\n",
    "    # Render the environment\n",
    "    draw_gridworld(grid, V=V)  # Optional: Visualize the state-value table (V)\n",
    "    draw_agent(agent_pos)\n",
    "    pygame.display.update()\n",
    "\n",
    "    # Training frequency control\n",
    "    if frame_count % training_step_frequency == 0:\n",
    "        training_active = True  # Trigger training step (optional)\n",
    "    else:\n",
    "        training_active = False\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Quit pygame and cleanup\n",
    "pygame.quit()\n",
    "\n",
    "# Save the learned policy (optional)\n",
    "with open(\"policy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(policy, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cfbd1-ff57-4037-b1df-30eef23bdee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2348e-d40b-4e47-8959-89fd2278f402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Assignment",
   "language": "python",
   "name": "rl_assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
