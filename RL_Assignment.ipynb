{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd24349-0084-4718-860c-3e60b2c1861d",
   "metadata": {},
   "source": [
    "# Step 1: Importing Libraries and Defining Constants\n",
    "\r\n",
    "In this step, we import necessary libraries such as pygame, numpy, pickle, and random. We also define constants for colors and gridworld dimension\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927aebad-4b71-4d13-8a7a-bbeb2460ebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Define colors\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 150, 0)  # Darker shade of green\n",
    "RED = (255, 0, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "# Gridworld dimensions\n",
    "n = 5  # Number of rows\n",
    "m = 5  # Number of columns\n",
    "\n",
    "# Font for text display (using default font)\n",
    "pygame.init()\n",
    "font = pygame.font.Font(None, 20)  # Using the default font"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a7381-11e0-466a-b9a0-0a6b027d6170",
   "metadata": {},
   "source": [
    "This step sets up the foundation for building the gridworld environment and defining its characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf58794-87af-47a0-920f-8cfbc1aef758",
   "metadata": {},
   "source": [
    "# Step 2: Initializing Pygame and Setting Up the Screen\n",
    "Here, we initialize Pygame and set up the screen for our gridworld environment. We create a Pygame window with a specific size to accommodate the grid and log area. Additionally, we set the window caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9bcbf4-9592-425e-87fe-66dafb327f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((650, 700))  # Set screen size to accommodate log area and wider grid\n",
    "pygame.display.set_caption(\"Gridworld Reinforcement Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ae656-d817-467f-bb9c-e359357bad12",
   "metadata": {},
   "source": [
    "This step prepares the environment for rendering the gridworld and displaying it to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e6f02-ee09-41e7-941f-d1f8866eee55",
   "metadata": {},
   "source": [
    "# Step 4: Drawing the Gridworld\r\n",
    "In this step, we define a function draw_gridworld() to draw the gridworld on the Pygame screen. This function takes the grid state grid and optionally the state value table V as input and visualizes the grid, rewards, values, and log entries.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da20004-5ea5-45f4-b357-0d5d57f7fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_gridworld(grid, V=None):\n",
    "    \"\"\"\n",
    "    Draws the gridworld on the screen with rewards, values, and log entries.\n",
    "    \"\"\"\n",
    "    screen.fill(WHITE)  # Fill screen with white color\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            # Draw border around each cell\n",
    "            pygame.draw.rect(screen, BLACK, (j * cell_size, i * cell_size, cell_size, cell_size), 1)\n",
    "\n",
    "            # Color cells based on grid values\n",
    "            if grid[i, j] == 'X':  # Obstacle\n",
    "                color = BLACK\n",
    "                pygame.draw.rect(screen, color, (j * cell_size, i * cell_size, cell_size, cell_size))  # Draw obstacle background\n",
    "            elif grid[i, j] == '1':  # Goal\n",
    "                color = GREEN\n",
    "                pygame.draw.rect(screen, GREEN, (j * cell_size + 1, i * cell_size + 1, cell_size - 2, cell_size - 2))  # Dark green for goal state\n",
    "            else:\n",
    "                color = WHITE\n",
    "\n",
    "            # Draw value on top of the grid\n",
    "            if V is not None:\n",
    "                value_text = font.render(str(round(V[i, j], 2)), True, BLACK)\n",
    "                text_rect = value_text.get_rect(center=(j * cell_size + cell_size // 2, i * cell_size + cell_size // 2))  # Center-align text\n",
    "                screen.blit(value_text, text_rect)\n",
    "\n",
    "            # Draw reward text in the center of the cell\n",
    "            reward_text = font.render(str(grid[i, j]), True, color)\n",
    "            reward_text_rect = reward_text.get_rect(center=(j * cell_size + cell_size // 2, i * cell_size + cell_size // 2))\n",
    "            screen.blit(reward_text, reward_text_rect)\n",
    "\n",
    "    # Draw log text\n",
    "    log_text = font.render(\"Log:\", True, BLACK)\n",
    "    screen.blit(log_text, (10, n * cell_size + 20))\n",
    "    log_area_height = 150\n",
    "    log_area = pygame.Rect(10, (n * cell_size + 50), 630, log_area_height)  # Define log area rectangle\n",
    "    pygame.draw.rect(screen, WHITE, log_area)  # Draw log area background\n",
    "    for index, entry in enumerate(log[-5:]):  # Display only the last 5 log entries\n",
    "        log_entry = font.render(entry, True, BLACK)\n",
    "        screen.blit(log_entry, (20, (n * cell_size + 70) + index * 25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4e3d5-ee7f-4ec1-955f-472e612c6446",
   "metadata": {},
   "source": [
    "This function is responsible for rendering the gridworld environment on the screen, including obstacles, goal states, rewards, and log entries. It provides a visual representation of the gridworld state to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff345345-80c9-455d-a0c0-3be466990d02",
   "metadata": {},
   "source": [
    "# Step 5: Drawing the Agent\r\n",
    "In this step, we define a function draw_agent() to draw the agent on the screen. The agent is represented as a red circle on the gridworld. Additionally, a small black dot is drawn at the center of the agent's cell for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d129624-686c-4f50-8024-d5849b52c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_agent(agent_pos):\n",
    "    \"\"\"\n",
    "    Draws the agent on the screen.\n",
    "    \"\"\"\n",
    "    pygame.draw.circle(screen, RED, (agent_pos[1] * cell_size + cell_size // 2,\n",
    "                                   agent_pos[0] * cell_size + cell_size // 2), cell_size // 3)\n",
    "    # Draw a dot in the center of the agent's cell\n",
    "    pygame.draw.circle(screen, BLACK, (agent_pos[1] * cell_size + cell_size // 2,\n",
    "                                     agent_pos[0] * cell_size + cell_size // 2), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0f60e-176b-490f-bf1f-fd132ada2f73",
   "metadata": {},
   "source": [
    "This function takes the agent's position agent_pos as input and draws a red circle representing the agent at the appropriate grid cell on the screen. Additionally, a small black dot is drawn at the center of the agent's cell for better visualization of the agent's position. This function is called during the rendering of the gridworld to display the agent's position to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f018f6c-a1d4-4e6d-be90-3c9d0809e63f",
   "metadata": {},
   "source": [
    "# Step 6: Determining Action Index\r\n",
    "Here, we define a function get_action_index() to determine the index corresponding to a given action. This function is useful for mapping actions to indices, which can be helpful for various operations, such as policy extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e8cf2bf-5628-4c99-b376-66103b868454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_index(action):\n",
    "    \"\"\"\n",
    "    Returns the index corresponding to a given action.\n",
    "    \"\"\"\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    return actions.index(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3ddfa-b6ab-4085-b14a-49127bb44964",
   "metadata": {},
   "source": [
    "The get_action_index() function takes an action as input and returns its corresponding index in the predefined list of actions. This index is used for various operations where actions need to be represented numerically, such as accessing elements in arrays or policy extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26df2a-767d-4550-84c2-41980242b2b6",
   "metadata": {},
   "source": [
    "# Step 7: Determining Valid Actions\r\n",
    "In this step, we define a function get_valid_actions() to determine the list of valid actions from a given state, considering obstacles in the grid. This function checks for valid actions by considering the boundaries of the grid and obstacles present in the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ad2d4b-d057-43a4-9b6a-ee1bca899484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(grid, state):\n",
    "    \"\"\"\n",
    "    Returns a list of valid actions from the given state (considering obstacles).\n",
    "    \"\"\"\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "    valid_actions = []\n",
    "    for action in actions:\n",
    "        new_pos = get_next_state(grid, state, action)\n",
    "        if new_pos is not None and grid[new_pos[0], new_pos[1]] != 'X':\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ec885-993c-4e0a-b9b0-8f5db3efd6ba",
   "metadata": {},
   "source": [
    "The get_valid_actions() function takes the grid and a state as input and returns a list of valid actions that can be taken from that state without hitting obstacles. It iterates over all possible actions and checks if moving in that direction would result in a valid position on the grid without hitting obstacles. If so, the action is added to the list of valid actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acbc80-c471-47a1-9dab-725e51e336b2",
   "metadata": {},
   "source": [
    "# Step 8: Value Iteration\r\n",
    "In this step, we implement the value_iteration() function to perform the Value Iteration algorithm. This algorithm calculates the state value table V, addressing invalid moves and revisiting penalty. The value_iteration() function iterates until convergence to update the state values based on the rewards and transitions in the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd64598b-2512-4461-bc7e-6467ddeab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, start, goal, discount_factor=0.9):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to calculate the state value table, addressing invalid moves and revisiting penalty.\n",
    "    \"\"\"\n",
    "    n = grid.shape[0]\n",
    "    m = grid.shape[1]\n",
    "    V = np.zeros((n, m))\n",
    "    state_visit_counts = {}  # Initialize dictionary to store visit counts for each state\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i, j] == 'X':\n",
    "                    V[i, j] = float('-inf')  # Set value to negative infinity for obstacle cells\n",
    "                    continue\n",
    "\n",
    "                v_old = V[i, j]\n",
    "                max_reward = float('-inf')  # Initialize maximum reward to negative infinity\n",
    "                valid_actions = get_valid_actions(grid, (i, j))\n",
    "\n",
    "                # Only consider valid actions to avoid invalid transitions\n",
    "                for action in valid_actions:\n",
    "                    new_pos = get_next_state(grid, (i, j), action)\n",
    "                    reward = float(grid[new_pos[0], new_pos[1]]) if new_pos != goal else 10.0  # Goal state reward\n",
    "                    reward += -1.0  # Step cost\n",
    "                    if grid[new_pos[0], new_pos[1]] == 'X':  # Penalty for obstacle\n",
    "                        reward -= 10.0\n",
    "                    # Stronger penalty for revisiting (adjust as needed)\n",
    "                    revisit_penalty = -2 * state_visit_counts.get((i, j), 0)\n",
    "                    total_reward = reward + revisit_penalty + discount_factor * V[new_pos[0], new_pos[1]]  # Total accumulated reward\n",
    "                    max_reward = max(max_reward, total_reward)  # Update maximum reward\n",
    "\n",
    "                # Update value based on maximum reward\n",
    "                V[i, j] = max_reward\n",
    "                delta = max(delta, abs(v_old - V[i, j]))\n",
    "\n",
    "                # Update state visit count (after updating V)\n",
    "                state = (i, j)\n",
    "                state_visit_counts[state] = state_visit_counts.get(state, 0) + 1\n",
    "\n",
    "        if delta < 1e-8:  # Epsilon for convergence criteria\n",
    "            break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68165ea8-6627-4619-8207-99ac6108015e",
   "metadata": {},
   "source": [
    "The value_iteration() function takes the grid, start state, goal state, and discount factor as input and returns the state value table V. It iterates over all states in the grid, updating their values based on the rewards, transitions, and the discount factor. This step is crucial for learning the optimal policy through reinforcement learning.\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c8447-d43c-497f-b76a-1386f9969aff",
   "metadata": {},
   "source": [
    "# Step 9: Determining Next State\r\n",
    "In this step, we define the get_next_state() function, which calculates the next state after taking an action from the current state. This function handles boundary conditions to ensure that the agent does not move out of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5078fc06-c46f-4874-9224-e7fe3b9926c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(grid, state, action):\n",
    "    \"\"\"\n",
    "    Returns the next state after taking an action from the current state,\n",
    "    handling boundary conditions.\n",
    "    \"\"\"\n",
    "    row_index, col_index = state  # Unpack state tuple\n",
    "\n",
    "    if action == 'up':\n",
    "        new_row_index = max(row_index - 1, 0)\n",
    "        new_col_index = col_index\n",
    "    elif action == 'down':\n",
    "        new_row_index = min(row_index + 1, n - 1)\n",
    "        new_col_index = col_index\n",
    "    elif action == 'left':\n",
    "        new_row_index = row_index\n",
    "        new_col_index = max(col_index - 1, 0)\n",
    "    elif action == 'right':\n",
    "        new_row_index = row_index\n",
    "        new_col_index = min(col_index + 1, m - 1)\n",
    "\n",
    "    if new_row_index < 0 or new_col_index < 0 or new_row_index >= n or new_col_index >= m:\n",
    "        return None  # Return None for invalid states (out of bounds)\n",
    "    else:\n",
    "        return (new_row_index, new_col_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f5283-4ba9-4316-9b82-9904eff902a7",
   "metadata": {},
   "source": [
    "The get_next_state() function takes the grid, current state, and action as input and returns the next state after applying the action. It handles boundary conditions to ensure that the agent remains within the gridworld. This function is essential for determining the agent's movement in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b6cd6-1d99-4900-9f34-4e3adc021561",
   "metadata": {},
   "source": [
    "# Step 10: Implementing Epsilon-Greedy Action Selection\r\n",
    "In this step, we define the get_action_with_exploration() function, which implements epsilon-greedy exploration with Boltzmann exploration for better spread. This function selects an action for the agent based on the current state and a given policy, considering exploration with a specified epsilon value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65284369-9a7d-465f-ac02-47b08bd19248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_with_exploration(state, policy, epsilon, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Implements Epsilon-greedy exploration with Boltzmann for better spread.\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_actions(grid, state)\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        action_probs = np.exp(np.array([V[state[0], state[1] + get_action_index(a)] for a in valid_actions]) / temperature)\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return random.choices(valid_actions, k=1, weights=action_probs)[0]\n",
    "    else:\n",
    "        return policy.get(state, random.choice(valid_actions))  # Prefer policy, fallback to random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d55537-f93b-4fe0-89f2-1778b57c3a99",
   "metadata": {},
   "source": [
    "This function takes the current state, policy, epsilon value, and optional temperature parameter as input and returns the selected action for the agent. If the random number generated is less than epsilon, it performs exploration by selecting an action probabilistically based on Boltzmann exploration. Otherwise, it chooses the action according to the policy. This function is crucial for balancing exploration and exploitation in the agent's decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f38919-c732-4b75-820b-66418256fc57",
   "metadata": {},
   "source": [
    "# Step 11: Defining the Gridworld Environment\r\n",
    "In this step, we initialize the gridworld environment by creating a placeholder grid represented as a NumPy array. This grid contains reward values and obstacles represented by specific symbols. The goal state is set to have a reward value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5351b0-ba8b-4e6b-83a2-86c47e60e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder grid\n",
    "grid = np.array([\n",
    "    [0.2, 0.3, 0.8, 0.1, 0],\n",
    "    [0.5, 'X', 0.7, 0.4, 0],\n",
    "    [0.9, 0.1, 0.6, 'X', 0.2],\n",
    "    [0.4, 'X', 0.3, 0.5, 0.8],\n",
    "    [0.7, 0.6, 0.1, 0.9, 1]  # Setting the goal state to 1\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13792e44-8a4e-415f-a5ab-5b43872e72e3",
   "metadata": {},
   "source": [
    "This placeholder grid serves as the environment in which the agent will navigate. It contains various reward values, obstacles represented by 'X', and a goal state with a reward value of 1. The grid's structure and reward values will influence the agent's learning process during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab42bdb-8c6d-4277-bb84-6f9848eae17e",
   "metadata": {},
   "source": [
    "# Step 12: Main Loop Initialization\r\n",
    "In this step, we initialize the main loop of our gridworld reinforcement learning environment. Several variables are defined to control the flow of the loop, including:<ul>\r",
    "<li>\n",
    "\r\n",
    "running: A boolean variable to control whether the main loop should continue runni </li>n<li>g.\r\n",
    "agent_pos: A tuple representing the initial position of the agent in the gridwo</li>r<li>ld.\r\n",
    "training_step_frequency: An integer defining how often the training step should be perfo</li>r<li>med.\r\n",
    "training_active: A boolean flag indicating whether the training step should be executed in the current iter</li>a<li>tion.\r\n",
    "frame_count: An integer to keep track of the number of frames processed in th</li>e<li> loop.\r\n",
    "Additionally, we define variables related to exploration and exploitation, such as epsilon, decay_rate, and temp</li>\n",
    "</ul>erature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d3a3e5-f282-445f-9039-5b8b62cf27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "running = True\n",
    "agent_pos = (0, 0)  # Initial agent position\n",
    "training_step_frequency = 10  # Train every 10 frames\n",
    "training_active = True  # Flag for training\n",
    "frame_count = 0\n",
    "\n",
    "# Additional variables for saving the model\n",
    "policy = {}  # Dictionary to store optimal actions for each state (learned from Value Iteration)\n",
    "model_saved = False  # Flag to track if the model is saved\n",
    "\n",
    "epsilon = 0.5  # Initial epsilon for exploration\n",
    "decay_rate = 0.95  # Initial decay rate for epsilon  # Steeper initial decay\n",
    "temperature = 1.0  # Temperature parameter for Boltzmann exploration (adjust as needed)\n",
    "\n",
    "status_text = font.render(\"Training Status:\", True, BLACK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d57a6-305a-4137-8b78-6cb529e8752d",
   "metadata": {},
   "source": [
    "# Step 13: Main Loop Execution\r\n",
    "In this step, we execute the main loop of our gridworld reinforcement learning environment. Within the loop, we handle events using Pygame's event system, such as checking for window close events to terminate the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8c374-5021-45bd-b9aa-a1df2e6be7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Update logic (separate training and visualization)\n",
    "    if training_active:\n",
    "        # Perform Value Iteration training step\n",
    "        V = value_iteration(grid.copy(), start=(0, 0), goal=(n - 1, m - 1))\n",
    "\n",
    "        # Extract optimal policy from the state value table (V)\n",
    "        policy = {}\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if grid[i, j] == 'X':\n",
    "                    continue\n",
    "                valid_actions = get_valid_actions(grid, (i, j))\n",
    "\n",
    "                # Choose action with exploration (using temperature)\n",
    "                best_action = get_action_with_exploration((i, j), policy, epsilon, temperature=temperature)\n",
    "\n",
    "                # ... (Rest of the policy extraction logic, similar to previous code)\n",
    "                # Here, update the policy dictionary with the chosen action for the current state\n",
    "\n",
    "    # Choose action based on the learned policy (with exploration using Epsilon-greedy)\n",
    "    current_action = policy.get(agent_pos, random.choice(['up', 'down', 'left', 'right']))\n",
    "\n",
    "    # Update agent position based on the action\n",
    "    new_pos = get_next_state(grid, agent_pos, current_action)\n",
    "    if new_pos != agent_pos:  # Check if the agent's position has changed\n",
    "        agent_pos = new_pos\n",
    "        # Log action, reward, and current state\n",
    "        reward = grid[agent_pos[0], agent_pos[1]]  # Obtain reward from grid\n",
    "        log.append(f\"Action: {current_action}, Total Reward: {reward}, Current State: {agent_pos}\")\n",
    "\n",
    "    # Render the environment\n",
    "    draw_gridworld(grid, V=V)  # Optional: Visualize the state-value table (V)\n",
    "    draw_agent(agent_pos)\n",
    "    pygame.display.flip()  # Update the entire display at once for better performance\n",
    "\n",
    "    # Training frequency control\n",
    "    if frame_count % training_step_frequency == 0:\n",
    "        training_active = True  # Trigger training step (optional)\n",
    "    else:\n",
    "        training_active = False\n",
    "\n",
    "    frame_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d75b31-6fe6-4c92-9d8d-8d3f33f4ba09",
   "metadata": {},
   "source": [
    "Inside the loop, we update the training logic and visualization separately. The training logic involves performing the Value Iteration step, extracting the optimal policy, and updating the agent's position based on the chosen action. The visualization part renders the gridworld environment and the agent's position on the screen using Pygame. Additionally, we control the frequency of training using the training_step_frequency variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb7c00-cc8a-4899-b55f-2f77f8c627f8",
   "metadata": {},
   "source": [
    "# Step 14: Quitting Pygame and Cleaning Up\r\n",
    "In this step, we handle the termination of the Pygame environment and perform cleanup operations before exiting the program. We use the pygame.quit() function to quit Pygame and release any resources used by the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07d6e2-7df6-4d1a-a238-e7b8d7ff7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quit pygame and cleanup\n",
    "pygame.quit()\n",
    "\n",
    "# Save the learned policy (optional)\n",
    "with open(\"policy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(policy, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43100ed2-2333-4f8d-9625-64cef54a449a",
   "metadata": {},
   "source": [
    "This step ensures that all resources allocated by Pygame are properly released and the program exits gracefully. Additionally, we have an optional step to save the learned policy to a file using pickle. This allows us to persist the learned policy for later use without having to retrain the model every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9068544-4706-416b-8ec2-39092917aca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Assignment",
   "language": "python",
   "name": "rl_assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
